{"cells":[{"cell_type":"markdown","metadata":{"id":"FS_uZL2Cd0rj"},"source":["# HORYZN x TRACER\n"]},{"cell_type":"markdown","metadata":{"id":"lZmkeqELbHvQ"},"source":["### TODO:\n","\n","1.   Load data \n","2.   Try model from one of the github repos against data\n","\n","Reference links:\n","* https://github.com/priya-dwivedi/aerial_pedestrian_detection\n","* https://github.com/prashantksharma/ee763\n","* https://github.com/karthik4444/nn-trajectory-prediction\n","* http://cs230.stanford.edu/projects_winter_2019/reports/15767730.pdf\n","* https://cs230.stanford.edu/projects_spring_2018/reports/8289901.pdf\n","* https://www.kaggle.com/aryashah2k/stanford-drone-dataset\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_FAD0QFSroMG"},"source":["https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/\n"]},{"cell_type":"markdown","metadata":{"id":"zJ75bL5Be-l-"},"source":["## 1. Loading Data\n","### Load data and vizualize annotations "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pA-acKxQaPhv"},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","from sklearn.preprocessing import LabelEncoder\n","# Change these 2 dirs if you store the data somewhere else.\n","annotations_folder = \"C:/projects/makeathon[tum.ai]/annotations/\"\n","videos_folder = \"C:/projects/makeathon[tum.ai]/video/\"\n","class DataLoader():\n","    def __init__(self, dataset_dic ):\n","        '''\n","        :param dataset_dic: A dictionary of dataset folder direction. the keys are the main folders names\n","        (e.g.: 'bookstore') and the values are a list of sub-folders (e.g.\"['video0]), [-1] is used when all\n","        subfolders are included.\n","        '''\n","        self.data_dic = dataset_dic\n","        self.data = []\n","        self.all_paths = []\n","        # Name of Objects\n","        self.objects = np.array([b'\"Biker\"',\n","                                 b'\"Bus\"',\n","                                 b'\"Car\"',\n","                                 b'\"Cart\"',\n","                                 b'\"Pedestrian\"',\n","                                 b'\"Skater\"'\n","                                 ])\n","        self.set_colors()\n","        self.load_data()\n","\n","    def set_colors(self):\n","        '''\n","        Assign an specific color to each object\n","        '''\n","        np.random.seed(1)\n","        self.colors = np.random.randint(0,255,(3,6), dtype=np.int32)\n","        np.random.seed()\n","\n","    def load_data(self):\n","        # collect all data paths\n","        for key in self.data_dic:\n","            if self.data_dic[key]==[-1]:\n","                folders = os.listdir(key)\n","                for folder in folders:\n","                    self.all_paths.append(os.path.join(key,folder))\n","            else:\n","                for folder in self.data_dic[key]:\n","                    self.all_paths.append(os.path.join(key, folder))\n","        # Assign a Number to each path and store it in a txt file. (later is used for visualization)\n","        f = open('dataset_idx.txt','w')\n","        for idx,path in enumerate(self.all_paths):\n","            f.write(str(idx)+' '+path+'\\n')\n","        f.close()\n","\n","        # Load Dataset and encode non-numerical features\n","        encoder = LabelEncoder()\n","        encoder.fit(self.objects)\n","        for idx, path in enumerate(self.all_paths):\n","            complete_path = os.path.join(annotations_folder,os.path.join(path, 'annotations.txt')).replace(\"\\\\\",\"/\")\n","            print(complete_path,' is loading.')\n","            raw_data = np.genfromtxt(complete_path, dtype=None )\n","            temp_data = np.zeros((raw_data.size, 10),dtype=np.int32)\n","            for column in range(10):\n","                if column == 9:\n","                    temp_data[:, column] = encoder.transform(raw_data[:]['f'+ str(column)])\n","                else:\n","                    temp_data[:, column] = raw_data[:]['f'+ str(column)]\n","                #raw_data = raw_data.astype(int)\n","            self.data.append(temp_data)\n","\n","    def visualize(self, dataset_idx):\n","        '''\n","        :param dataset_idx: the idx of selected data for visualization\n","        '''\n","        path = self.all_paths[dataset_idx]\n","        annotation_data = self.data[dataset_idx]\n","        video_dir = os.path.join(videos_folder,os.path.join(path,'video.mp4'))\n","        cap = cv2.VideoCapture(video_dir)\n","        frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        print('frame width: ', frameWidth)\n","        print('frame height: ',frameHeight)\n","        fc = 0\n","        ret = True\n","\n","        while ret:\n","            ret, img = cap.read()\n","            frame_objects = annotation_data[annotation_data[:,5]==fc,:]\n","            for i in range(len(frame_objects)):\n","                color = (int(self.colors[0,frame_objects[i,-1]]),int(self.colors[1,frame_objects[i,-1]]),int(self.colors[2,frame_objects[i,-1]]))\n","                cv2.rectangle(img, (frame_objects[i,1], frame_objects[i,2]), (frame_objects[i,3], frame_objects[i,4]), color=color, thickness=4)\n","                cv2.putText(img, str(frame_objects[i,0]), (frame_objects[i,1], frame_objects[i,2]), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n","            # Press esc to exit\n","            k = cv2.waitKey(30) & 0xff\n","            if k == 27:\n","                break\n","            fc = fc + 1\n","            # frame Width and Height is divided by 2 to fit in my screen.\n","            img = cv2.resize(img, (int(frameWidth/2), int(frameHeight/2)))\n","            \n","            cv2.imshow('BookStore', img)\n","            \n","        cap.release()\n","        cv2.destroyAllWindows()\n","\n","\n","if __name__==\"__main__\":\n","    data_dic = {'Bookstore':['video1','video2']}\n","                #'coupa':[-1],\n","                #'deathCircle':[-1],\n","                #'gates':[-1],\n","                #'hyang':[-1],\n","                #'little':[-1],\n","                #'nexus':[-1],\n","                #'quad':[-1]}\n","    stanford_vis = DataLoader(data_dic)\n","    stanford_vis.visualize(dataset_idx = 1)"]},{"cell_type":"markdown","metadata":{"id":"bREXQOlVfCO3"},"source":["## 2. Trying Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-Ebk_jCy6f4"},"outputs":[],"source":["import cv2 \n","import matplotlib.pyplot as plt\n","img = plt.imread('frame158.jpg')   # reading image using its name\n","plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"q9fWjXkey_cj"},"source":["1.   Track ID. All rows with the same ID belong to the same path.\n","2.   xmin. The top left x-coordinate of the bounding box.\n","3.   ymin. The top left y-coordinate of the bounding box.\n","4.   xmax. The bottom right x-coordinate of the bounding box.\n","5.   ymax. The bottom right y-coordinate of the bounding box.\n","6.   frame. The frame that this annotation represents.\n","7.   lost. If 1, the annotation is outside of the view screen.\n","8.   occluded. If 1, the annotation is occluded.\n","9.   generated. If 1, the annotation was automatically interpolated.\n","10.  label. The label for this annotation, enclosed in quotation marks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqe-eI16y2_F"},"outputs":[],"source":["import pandas as pd\n","\n","txtFile = \"C:/projects/makeathon[tum.ai]/annotations/coupa/video1/annotations.txt\"\n","names = [\"Track ID\",\"xmin\",'ymin','xmax','ymax','frame','lost','occluded','generated','label']\n","data = pd.read_csv(txtFile, delimiter=\" \",header=None,names=names)\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_Azy-XlclgY"},"outputs":[],"source":["im = cv2.imread(\"frame158.jpg\")\n","gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n","#contours, hierarchy = cv2.findContours(gray,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n","\n","\n","idx =0 \n","for cnt in contours:\n","    idx += 1\n","    x,y,w,h = cv2.boundingRect(cnt)\n","    roi=im[y:y+h,x:x+w]\n","cv2.imwrite(str(idx) + '.jpg', roi)\n","cv2.rectangle(im,(data['xmin'][158],\n","                  data['ymin'][158]),\n","              (data['xmax'][158],\n","               data['ymax'][158]), (200,0,0),2)\n","cv2.imshow('img',im)\n","cv2.waitKey(0)"]},{"cell_type":"markdown","metadata":{"id":"_6N_SrFexw7i"},"source":["#"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"HORYZNxTRACER.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
